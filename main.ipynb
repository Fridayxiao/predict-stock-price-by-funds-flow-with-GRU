{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 10s 101ms/step - loss: 14.7695 - mse: 14.7695 - val_loss: 5.9841 - val_mse: 5.9841\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 4s 71ms/step - loss: 3.4253 - mse: 3.4253 - val_loss: 5.8880 - val_mse: 5.8880\n",
      "Epoch 3/10\n",
      " 6/50 [==>...........................] - ETA: 3s - loss: 3.8575 - mse: 3.8575"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 132\u001b[0m\n\u001b[0;32m    128\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    129\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    131\u001b[0m \u001b[39m# 训练模型\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m    133\u001b[0m           validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[0;32m    135\u001b[0m \u001b[39m# 模型预测\u001b[39;00m\n\u001b[0;32m    136\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\engine\\training.py:1389\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1387\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1388\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1389\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1391\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\callbacks.py:438\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 438\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\callbacks.py:297\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    295\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    296\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 297\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    300\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\callbacks.py:318\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    316\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 318\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    320\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    321\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\callbacks.py:356\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    355\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 356\u001b[0m   hook(batch, logs)\n\u001b[0;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    359\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\callbacks.py:1034\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1034\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\callbacks.py:1107\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1105\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m-> 1107\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\utils\\generic_utils.py:976\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    973\u001b[0m     info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    975\u001b[0m   message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[1;32m--> 976\u001b[0m   io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    977\u001b[0m   message \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    979\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\keras\\utils\\io_utils.py:37\u001b[0m, in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     35\u001b[0m   sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m   sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mwrite(message)\n\u001b[0;32m     38\u001b[0m sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mflush()\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\ipykernel\\iostream.py:574\u001b[0m, in \u001b[0;36mOutStream.write\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush)\n\u001b[0;32m    573\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_schedule_flush()\n\u001b[0;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(string)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\ipykernel\\iostream.py:478\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_schedule_in_thread\u001b[39m():\n\u001b[0;32m    476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_io_loop\u001b[39m.\u001b[39mcall_later(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflush_interval, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flush)\n\u001b[1;32m--> 478\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpub_thread\u001b[39m.\u001b[39;49mschedule(_schedule_in_thread)\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\ipykernel\\iostream.py:211\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_events\u001b[39m.\u001b[39mappend(f)\n\u001b[0;32m    210\u001b[0m     \u001b[39m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_pipe\u001b[39m.\u001b[39;49msend(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    212\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     f()\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\zmq\\sugar\\socket.py:688\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    681\u001b[0m         data \u001b[39m=\u001b[39m zmq\u001b[39m.\u001b[39mFrame(\n\u001b[0;32m    682\u001b[0m             data,\n\u001b[0;32m    683\u001b[0m             track\u001b[39m=\u001b[39mtrack,\n\u001b[0;32m    684\u001b[0m             copy\u001b[39m=\u001b[39mcopy \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    685\u001b[0m             copy_threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_threshold,\n\u001b[0;32m    686\u001b[0m         )\n\u001b[0;32m    687\u001b[0m     data\u001b[39m.\u001b[39mgroup \u001b[39m=\u001b[39m group\n\u001b[1;32m--> 688\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(data, flags\u001b[39m=\u001b[39;49mflags, copy\u001b[39m=\u001b[39;49mcopy, track\u001b[39m=\u001b[39;49mtrack)\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Study\\neuralnetwork\\d2l\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "response = requests.get(\n",
    "    \"http://vip.stock.finance.sina.com.cn/quotes_service/api/json_v2.php/MoneyFlow.ssl_qsfx_lscjfb?page=1&num=1000&sort=opendate&asc=0&daima=sz002057\",\n",
    ")\n",
    "\n",
    "# 通过json.loads()方法将json字符串转换为python对象\n",
    "\n",
    "a = json.loads(response.text)\n",
    "# 将a转换为DataFrame\n",
    "\n",
    "data = pd.DataFrame(a)\n",
    "data.drop(['r0', 'r1', 'r2', 'r3'], axis=1, inplace=True)\n",
    "\n",
    "# 修改列名\n",
    "data.columns = [\"日期\", \"收盘价\", \"涨跌幅\", \"换手率\", \"净流入额\",\n",
    "                \"净流入占比\", \"超大单净流入\", \"大单净流入\", \"小单净流入\", \"散单净流入\"]\n",
    "#\n",
    "# 修改数据类型\n",
    "data['日期'] = pd.to_datetime(data['日期'])\n",
    "data['收盘价'] = data['收盘价'].astype(float)\n",
    "data['涨跌幅'] = data['涨跌幅'].astype(float)\n",
    "data['换手率'] = data['换手率'].astype(float)\n",
    "data['净流入额'] = data['净流入额'].astype(float)\n",
    "data['净流入占比'] = data['净流入占比'].astype(float)\n",
    "data['超大单净流入'] = data['超大单净流入'].astype(float)\n",
    "data['大单净流入'] = data['大单净流入'].astype(float)\n",
    "data['小单净流入'] = data['小单净流入'].astype(float)\n",
    "data['散单净流入'] = data['散单净流入'].astype(float)\n",
    "data = data.sort_values(by='日期', ascending=True)\n",
    "# 将日期设置为索引\n",
    "data.set_index('日期', inplace=True)\n",
    "data.head()\n",
    "\n",
    "\n",
    "# 使用GRU对涨幅单步预测\n",
    "\n",
    "# 数据预处理\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 9\n",
    "seq_len = 30\n",
    "# 使用滑动窗口将数据转换为时间序列数据\n",
    "\n",
    "\n",
    "def sliding_window(DataSet, X_width, y_width, gap=1, multi_vector=None, X_data=True):\n",
    "    '''\n",
    "    DataSet has to be as a DataFrame\n",
    "    '''\n",
    "    if X_data:\n",
    "        if multi_vector:\n",
    "            a, b = DataSet.shape\n",
    "        else:\n",
    "            a = DataSet.shape[0]\n",
    "            b = 1\n",
    "        c = (a-X_width-y_width-a % gap)/gap\n",
    "        X = np.reshape(DataSet.iloc[0:X_width, :].values, (1, X_width, b))\n",
    "        for i in range(len(DataSet) - X_width - y_width):\n",
    "            i += 1\n",
    "            if i > c:\n",
    "                break\n",
    "            j = i * gap\n",
    "            tmp = DataSet.iloc[j:j + X_width, :].values\n",
    "            tmp = np.reshape(tmp, (1, X_width, b))\n",
    "            X = np.concatenate([X, tmp], 0)\n",
    "        return X\n",
    "    else:\n",
    "        if multi_vector:\n",
    "            print('y_data-error：expect 1D ,given %dD' % DataSet.shape[1])\n",
    "            return\n",
    "        else:\n",
    "            a = DataSet.shape[0]\n",
    "        c = (a-X_width-y_width-a % gap)/gap\n",
    "        y = np.reshape(\n",
    "            DataSet.iloc[X_width:X_width + y_width].values, (1, y_width))\n",
    "        for i in range(len(DataSet) - X_width - y_width):\n",
    "            i += 1\n",
    "            if i > c:\n",
    "                break\n",
    "            j = i * gap + X_width\n",
    "            tmp = DataSet.iloc[j:j + y_width].values\n",
    "            tmp = np.reshape(tmp, (1, y_width))\n",
    "            y = np.concatenate([y, tmp])\n",
    "        return y\n",
    "\n",
    "\n",
    "X = sliding_window(data, seq_len, 1, 1, True, True)\n",
    "\n",
    "y = sliding_window(data[\"收盘价\"], seq_len, 1, 1, multi_vector=False, X_data=None)\n",
    "\n",
    "# 标准化\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x = (x - x.mean()) / x.std()\n",
    "    return x\n",
    "\n",
    "\n",
    "train_rate = 0.8\n",
    "train_num = int(n_samples * train_rate)\n",
    "\n",
    "\n",
    "X_train = np.array(X[:train_num])\n",
    "X_test = np.array(X[train_num:])\n",
    "\n",
    "y_train = y[:train_num]\n",
    "y_test = y[train_num:]\n",
    "# 标准化训练集与测试集\n",
    "X_train = np.apply_along_axis(normalize, axis=1, arr=X_train)\n",
    "X_test = np.apply_along_axis(normalize, axis=1, arr=X_test)\n",
    "# 定义模型结构\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.GRU(256, input_shape=(seq_len, n_features),\n",
    "                        return_sequences=True, dropout=0.2),\n",
    "    tf.keras.layers.GRU(256, return_sequences=True, dropout=0.4),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True, dropout=0.2),\n",
    "    tf.keras.layers.GRU(256, dropout=0.2),\n",
    "    tf.keras.layers.Dense(1),\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='mse',\n",
    "              metrics=['mse'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "# 模型预测\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# 模型评估\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', loss)\n",
    "\n",
    "\n",
    "preds.shape\n",
    "\n",
    "# 画图并显示图例\n",
    "\n",
    "plt.plot(preds, label='predit',)\n",
    "plt.plot(y_test, label='real',)\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bbf74d50f710abf8fd293a136a4a2bb5e2b2276658eb36166a279a6d0d425cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
